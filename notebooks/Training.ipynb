{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aditya/git/RCNN_Pneumonia\n",
      "env: PROJECT_PATH=/home/aditya/git/RCNN_Pneumonia\n"
     ]
    }
   ],
   "source": [
    "%cd /home/aditya/git/RCNN_Pneumonia\n",
    "%env PROJECT_PATH /home/aditya/git/RCNN_Pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.envs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.dataset import PneumoniaDataset\n",
    "from model.config import PneumoniaConfig\n",
    "from model.augmentation import augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from mrcnn import model as modellib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PneumoniaDataset(data_dir, 'train')\n",
    "dev_dataset = PneumoniaDataset(data_dir, 'dev')\n",
    "val_dataset = PneumoniaDataset(data_dir, 'val')\n",
    "test_dataset = PneumoniaDataset(data_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.prepare()\n",
    "dev_dataset.prepare()\n",
    "val_dataset.prepare()\n",
    "test_dataset.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PneumoniaConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(coco_weights_path, by_name=True, exclude=[\n",
    "    \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n",
    "    \"mrcnn_bbox\", \"mrcnn_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train network heads\n",
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /home/aditya/git/RCNN_Pneumonia/logs/pneumonia20181023T0029/mask_rcnn_pneumonia_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/aditya/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py:46: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 770s 770ms/step - loss: 1.6272 - rpn_class_loss: 0.0082 - rpn_bbox_loss: 0.5156 - mrcnn_class_loss: 0.1751 - mrcnn_bbox_loss: 0.5228 - mrcnn_mask_loss: 0.4054 - val_loss: 1.5590 - val_rpn_class_loss: 0.0070 - val_rpn_bbox_loss: 0.5250 - val_mrcnn_class_loss: 0.1554 - val_mrcnn_bbox_loss: 0.4729 - val_mrcnn_mask_loss: 0.3987\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 741s 741ms/step - loss: 1.3391 - rpn_class_loss: 0.0057 - rpn_bbox_loss: 0.4089 - mrcnn_class_loss: 0.1249 - mrcnn_bbox_loss: 0.4214 - mrcnn_mask_loss: 0.3782 - val_loss: 1.5369 - val_rpn_class_loss: 0.0057 - val_rpn_bbox_loss: 0.3892 - val_mrcnn_class_loss: 0.1667 - val_mrcnn_bbox_loss: 0.5382 - val_mrcnn_mask_loss: 0.4372\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 747s 747ms/step - loss: 1.3020 - rpn_class_loss: 0.0050 - rpn_bbox_loss: 0.4063 - mrcnn_class_loss: 0.1138 - mrcnn_bbox_loss: 0.4004 - mrcnn_mask_loss: 0.3764 - val_loss: 1.2665 - val_rpn_class_loss: 0.0043 - val_rpn_bbox_loss: 0.4429 - val_mrcnn_class_loss: 0.1014 - val_mrcnn_bbox_loss: 0.3583 - val_mrcnn_mask_loss: 0.3596\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 751s 751ms/step - loss: 1.2370 - rpn_class_loss: 0.0048 - rpn_bbox_loss: 0.3763 - mrcnn_class_loss: 0.1026 - mrcnn_bbox_loss: 0.3809 - mrcnn_mask_loss: 0.3724 - val_loss: 1.6281 - val_rpn_class_loss: 0.0077 - val_rpn_bbox_loss: 0.7903 - val_mrcnn_class_loss: 0.0778 - val_mrcnn_bbox_loss: 0.3903 - val_mrcnn_mask_loss: 0.3619\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 757s 757ms/step - loss: 1.2086 - rpn_class_loss: 0.0044 - rpn_bbox_loss: 0.3712 - mrcnn_class_loss: 0.0891 - mrcnn_bbox_loss: 0.3689 - mrcnn_mask_loss: 0.3749 - val_loss: 1.7697 - val_rpn_class_loss: 0.0081 - val_rpn_bbox_loss: 0.8883 - val_mrcnn_class_loss: 0.0651 - val_mrcnn_bbox_loss: 0.3931 - val_mrcnn_mask_loss: 0.4151\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 755s 755ms/step - loss: 1.1655 - rpn_class_loss: 0.0044 - rpn_bbox_loss: 0.3497 - mrcnn_class_loss: 0.0889 - mrcnn_bbox_loss: 0.3552 - mrcnn_mask_loss: 0.3673 - val_loss: 1.2930 - val_rpn_class_loss: 0.0036 - val_rpn_bbox_loss: 0.3384 - val_mrcnn_class_loss: 0.1348 - val_mrcnn_bbox_loss: 0.4367 - val_mrcnn_mask_loss: 0.3795\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 753s 753ms/step - loss: 1.1705 - rpn_class_loss: 0.0038 - rpn_bbox_loss: 0.3575 - mrcnn_class_loss: 0.0853 - mrcnn_bbox_loss: 0.3545 - mrcnn_mask_loss: 0.3694 - val_loss: 1.1372 - val_rpn_class_loss: 0.0039 - val_rpn_bbox_loss: 0.3206 - val_mrcnn_class_loss: 0.0916 - val_mrcnn_bbox_loss: 0.3480 - val_mrcnn_mask_loss: 0.3731\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 749s 749ms/step - loss: 1.1869 - rpn_class_loss: 0.0041 - rpn_bbox_loss: 0.3390 - mrcnn_class_loss: 0.1011 - mrcnn_bbox_loss: 0.3647 - mrcnn_mask_loss: 0.3779 - val_loss: 1.2722 - val_rpn_class_loss: 0.0039 - val_rpn_bbox_loss: 0.4349 - val_mrcnn_class_loss: 0.0799 - val_mrcnn_bbox_loss: 0.3726 - val_mrcnn_mask_loss: 0.3808\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 754s 754ms/step - loss: 1.1614 - rpn_class_loss: 0.0044 - rpn_bbox_loss: 0.3641 - mrcnn_class_loss: 0.0837 - mrcnn_bbox_loss: 0.3464 - mrcnn_mask_loss: 0.3629 - val_loss: 1.4703 - val_rpn_class_loss: 0.0055 - val_rpn_bbox_loss: 0.6139 - val_mrcnn_class_loss: 0.0539 - val_mrcnn_bbox_loss: 0.4044 - val_mrcnn_mask_loss: 0.3924\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 747s 747ms/step - loss: 1.1188 - rpn_class_loss: 0.0040 - rpn_bbox_loss: 0.3337 - mrcnn_class_loss: 0.0792 - mrcnn_bbox_loss: 0.3400 - mrcnn_mask_loss: 0.3619 - val_loss: 1.1699 - val_rpn_class_loss: 0.0038 - val_rpn_bbox_loss: 0.3466 - val_mrcnn_class_loss: 0.1150 - val_mrcnn_bbox_loss: 0.3488 - val_mrcnn_mask_loss: 0.3556\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 745s 745ms/step - loss: 1.1323 - rpn_class_loss: 0.0039 - rpn_bbox_loss: 0.3306 - mrcnn_class_loss: 0.0945 - mrcnn_bbox_loss: 0.3407 - mrcnn_mask_loss: 0.3626 - val_loss: 1.3272 - val_rpn_class_loss: 0.0042 - val_rpn_bbox_loss: 0.4238 - val_mrcnn_class_loss: 0.1070 - val_mrcnn_bbox_loss: 0.4050 - val_mrcnn_mask_loss: 0.3873\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 739s 739ms/step - loss: 1.1334 - rpn_class_loss: 0.0042 - rpn_bbox_loss: 0.3275 - mrcnn_class_loss: 0.0976 - mrcnn_bbox_loss: 0.3424 - mrcnn_mask_loss: 0.3618 - val_loss: 1.3132 - val_rpn_class_loss: 0.0050 - val_rpn_bbox_loss: 0.3304 - val_mrcnn_class_loss: 0.1688 - val_mrcnn_bbox_loss: 0.4029 - val_mrcnn_mask_loss: 0.4062\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 744s 744ms/step - loss: 1.0980 - rpn_class_loss: 0.0038 - rpn_bbox_loss: 0.3138 - mrcnn_class_loss: 0.0855 - mrcnn_bbox_loss: 0.3341 - mrcnn_mask_loss: 0.3607 - val_loss: 1.2198 - val_rpn_class_loss: 0.0046 - val_rpn_bbox_loss: 0.3885 - val_mrcnn_class_loss: 0.0945 - val_mrcnn_bbox_loss: 0.3633 - val_mrcnn_mask_loss: 0.3688\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 734s 734ms/step - loss: 1.1219 - rpn_class_loss: 0.0037 - rpn_bbox_loss: 0.3272 - mrcnn_class_loss: 0.0867 - mrcnn_bbox_loss: 0.3353 - mrcnn_mask_loss: 0.3688 - val_loss: 1.4704 - val_rpn_class_loss: 0.0060 - val_rpn_bbox_loss: 0.4762 - val_mrcnn_class_loss: 0.1042 - val_mrcnn_bbox_loss: 0.4450 - val_mrcnn_mask_loss: 0.4390\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 734s 734ms/step - loss: 1.0806 - rpn_class_loss: 0.0034 - rpn_bbox_loss: 0.3016 - mrcnn_class_loss: 0.0877 - mrcnn_bbox_loss: 0.3279 - mrcnn_mask_loss: 0.3600 - val_loss: 1.2771 - val_rpn_class_loss: 0.0033 - val_rpn_bbox_loss: 0.3582 - val_mrcnn_class_loss: 0.0935 - val_mrcnn_bbox_loss: 0.4170 - val_mrcnn_mask_loss: 0.4051\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 743s 743ms/step - loss: 1.0448 - rpn_class_loss: 0.0036 - rpn_bbox_loss: 0.2829 - mrcnn_class_loss: 0.0816 - mrcnn_bbox_loss: 0.3214 - mrcnn_mask_loss: 0.3553 - val_loss: 1.3062 - val_rpn_class_loss: 0.0049 - val_rpn_bbox_loss: 0.5032 - val_mrcnn_class_loss: 0.1284 - val_mrcnn_bbox_loss: 0.3311 - val_mrcnn_mask_loss: 0.3386\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 735s 735ms/step - loss: 1.0625 - rpn_class_loss: 0.0034 - rpn_bbox_loss: 0.3039 - mrcnn_class_loss: 0.0789 - mrcnn_bbox_loss: 0.3204 - mrcnn_mask_loss: 0.3560 - val_loss: 1.2936 - val_rpn_class_loss: 0.0045 - val_rpn_bbox_loss: 0.3967 - val_mrcnn_class_loss: 0.1083 - val_mrcnn_bbox_loss: 0.3977 - val_mrcnn_mask_loss: 0.3864\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 724s 724ms/step - loss: 1.0868 - rpn_class_loss: 0.0035 - rpn_bbox_loss: 0.3139 - mrcnn_class_loss: 0.0895 - mrcnn_bbox_loss: 0.3242 - mrcnn_mask_loss: 0.3557 - val_loss: 1.2172 - val_rpn_class_loss: 0.0053 - val_rpn_bbox_loss: 0.3710 - val_mrcnn_class_loss: 0.0876 - val_mrcnn_bbox_loss: 0.3850 - val_mrcnn_mask_loss: 0.3682\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 729s 729ms/step - loss: 1.0408 - rpn_class_loss: 0.0031 - rpn_bbox_loss: 0.2881 - mrcnn_class_loss: 0.0820 - mrcnn_bbox_loss: 0.3142 - mrcnn_mask_loss: 0.3533 - val_loss: 1.5819 - val_rpn_class_loss: 0.0060 - val_rpn_bbox_loss: 0.8771 - val_mrcnn_class_loss: 0.0523 - val_mrcnn_bbox_loss: 0.3211 - val_mrcnn_mask_loss: 0.3255\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 733s 733ms/step - loss: 1.0238 - rpn_class_loss: 0.0034 - rpn_bbox_loss: 0.2798 - mrcnn_class_loss: 0.0794 - mrcnn_bbox_loss: 0.3072 - mrcnn_mask_loss: 0.3540 - val_loss: 1.1230 - val_rpn_class_loss: 0.0036 - val_rpn_bbox_loss: 0.3268 - val_mrcnn_class_loss: 0.0801 - val_mrcnn_bbox_loss: 0.3366 - val_mrcnn_mask_loss: 0.3759\n"
     ]
    }
   ],
   "source": [
    "model.train(dev_dataset, val_dataset,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=20,\n",
    "            layers='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.keras_model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_rpn_class_loss</th>\n",
       "      <th>val_rpn_bbox_loss</th>\n",
       "      <th>val_mrcnn_class_loss</th>\n",
       "      <th>val_mrcnn_bbox_loss</th>\n",
       "      <th>val_mrcnn_mask_loss</th>\n",
       "      <th>loss</th>\n",
       "      <th>rpn_class_loss</th>\n",
       "      <th>rpn_bbox_loss</th>\n",
       "      <th>mrcnn_class_loss</th>\n",
       "      <th>mrcnn_bbox_loss</th>\n",
       "      <th>mrcnn_mask_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.559037</td>\n",
       "      <td>0.006998</td>\n",
       "      <td>0.524973</td>\n",
       "      <td>0.155440</td>\n",
       "      <td>0.472899</td>\n",
       "      <td>0.398720</td>\n",
       "      <td>1.627178</td>\n",
       "      <td>0.008232</td>\n",
       "      <td>0.515597</td>\n",
       "      <td>0.175124</td>\n",
       "      <td>0.522818</td>\n",
       "      <td>0.405400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.536923</td>\n",
       "      <td>0.005690</td>\n",
       "      <td>0.389247</td>\n",
       "      <td>0.166668</td>\n",
       "      <td>0.538154</td>\n",
       "      <td>0.437156</td>\n",
       "      <td>1.339134</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.408866</td>\n",
       "      <td>0.124896</td>\n",
       "      <td>0.421435</td>\n",
       "      <td>0.378224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.266506</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>0.442891</td>\n",
       "      <td>0.101443</td>\n",
       "      <td>0.358257</td>\n",
       "      <td>0.359615</td>\n",
       "      <td>1.301982</td>\n",
       "      <td>0.005006</td>\n",
       "      <td>0.406349</td>\n",
       "      <td>0.113762</td>\n",
       "      <td>0.400441</td>\n",
       "      <td>0.376416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.628055</td>\n",
       "      <td>0.007729</td>\n",
       "      <td>0.790306</td>\n",
       "      <td>0.077846</td>\n",
       "      <td>0.390278</td>\n",
       "      <td>0.361887</td>\n",
       "      <td>1.237034</td>\n",
       "      <td>0.004815</td>\n",
       "      <td>0.376350</td>\n",
       "      <td>0.102587</td>\n",
       "      <td>0.380855</td>\n",
       "      <td>0.372419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.769657</td>\n",
       "      <td>0.008058</td>\n",
       "      <td>0.888261</td>\n",
       "      <td>0.065135</td>\n",
       "      <td>0.393145</td>\n",
       "      <td>0.415052</td>\n",
       "      <td>1.208559</td>\n",
       "      <td>0.004360</td>\n",
       "      <td>0.371225</td>\n",
       "      <td>0.089148</td>\n",
       "      <td>0.368944</td>\n",
       "      <td>0.374875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.292980</td>\n",
       "      <td>0.003645</td>\n",
       "      <td>0.338385</td>\n",
       "      <td>0.134752</td>\n",
       "      <td>0.436658</td>\n",
       "      <td>0.379533</td>\n",
       "      <td>1.165507</td>\n",
       "      <td>0.004371</td>\n",
       "      <td>0.349712</td>\n",
       "      <td>0.088900</td>\n",
       "      <td>0.355226</td>\n",
       "      <td>0.367290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.137150</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.320562</td>\n",
       "      <td>0.091569</td>\n",
       "      <td>0.347974</td>\n",
       "      <td>0.373113</td>\n",
       "      <td>1.170505</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>0.357518</td>\n",
       "      <td>0.085255</td>\n",
       "      <td>0.354488</td>\n",
       "      <td>0.369419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.272166</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.434949</td>\n",
       "      <td>0.079931</td>\n",
       "      <td>0.372579</td>\n",
       "      <td>0.380801</td>\n",
       "      <td>1.186856</td>\n",
       "      <td>0.004057</td>\n",
       "      <td>0.339044</td>\n",
       "      <td>0.101099</td>\n",
       "      <td>0.364740</td>\n",
       "      <td>0.377908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.470298</td>\n",
       "      <td>0.005524</td>\n",
       "      <td>0.613948</td>\n",
       "      <td>0.053944</td>\n",
       "      <td>0.404428</td>\n",
       "      <td>0.392447</td>\n",
       "      <td>1.161433</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>0.364094</td>\n",
       "      <td>0.083685</td>\n",
       "      <td>0.346381</td>\n",
       "      <td>0.362877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.169950</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.346621</td>\n",
       "      <td>0.115049</td>\n",
       "      <td>0.348811</td>\n",
       "      <td>0.355614</td>\n",
       "      <td>1.118763</td>\n",
       "      <td>0.004042</td>\n",
       "      <td>0.333691</td>\n",
       "      <td>0.079176</td>\n",
       "      <td>0.339988</td>\n",
       "      <td>0.361858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.327212</td>\n",
       "      <td>0.004178</td>\n",
       "      <td>0.423819</td>\n",
       "      <td>0.106955</td>\n",
       "      <td>0.404976</td>\n",
       "      <td>0.387276</td>\n",
       "      <td>1.132341</td>\n",
       "      <td>0.003851</td>\n",
       "      <td>0.330630</td>\n",
       "      <td>0.094548</td>\n",
       "      <td>0.340662</td>\n",
       "      <td>0.362642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.313162</td>\n",
       "      <td>0.004971</td>\n",
       "      <td>0.330364</td>\n",
       "      <td>0.168774</td>\n",
       "      <td>0.402856</td>\n",
       "      <td>0.406189</td>\n",
       "      <td>1.133428</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>0.327471</td>\n",
       "      <td>0.097593</td>\n",
       "      <td>0.342382</td>\n",
       "      <td>0.361766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.219781</td>\n",
       "      <td>0.004627</td>\n",
       "      <td>0.388499</td>\n",
       "      <td>0.094531</td>\n",
       "      <td>0.363277</td>\n",
       "      <td>0.368839</td>\n",
       "      <td>1.097999</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>0.313833</td>\n",
       "      <td>0.085535</td>\n",
       "      <td>0.334100</td>\n",
       "      <td>0.360748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.470429</td>\n",
       "      <td>0.005969</td>\n",
       "      <td>0.476210</td>\n",
       "      <td>0.104199</td>\n",
       "      <td>0.445033</td>\n",
       "      <td>0.439010</td>\n",
       "      <td>1.121878</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>0.327246</td>\n",
       "      <td>0.086727</td>\n",
       "      <td>0.335324</td>\n",
       "      <td>0.368830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.277091</td>\n",
       "      <td>0.003263</td>\n",
       "      <td>0.358205</td>\n",
       "      <td>0.093495</td>\n",
       "      <td>0.416985</td>\n",
       "      <td>0.405136</td>\n",
       "      <td>1.080598</td>\n",
       "      <td>0.003447</td>\n",
       "      <td>0.301585</td>\n",
       "      <td>0.087673</td>\n",
       "      <td>0.327861</td>\n",
       "      <td>0.360024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.306181</td>\n",
       "      <td>0.004869</td>\n",
       "      <td>0.503184</td>\n",
       "      <td>0.128378</td>\n",
       "      <td>0.331120</td>\n",
       "      <td>0.338623</td>\n",
       "      <td>1.044755</td>\n",
       "      <td>0.003629</td>\n",
       "      <td>0.282868</td>\n",
       "      <td>0.081565</td>\n",
       "      <td>0.321409</td>\n",
       "      <td>0.355275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.293567</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>0.396711</td>\n",
       "      <td>0.108286</td>\n",
       "      <td>0.397661</td>\n",
       "      <td>0.386413</td>\n",
       "      <td>1.062506</td>\n",
       "      <td>0.003362</td>\n",
       "      <td>0.303931</td>\n",
       "      <td>0.078861</td>\n",
       "      <td>0.320375</td>\n",
       "      <td>0.355969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.217158</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>0.371032</td>\n",
       "      <td>0.087607</td>\n",
       "      <td>0.384959</td>\n",
       "      <td>0.368228</td>\n",
       "      <td>1.086805</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>0.313893</td>\n",
       "      <td>0.089518</td>\n",
       "      <td>0.324235</td>\n",
       "      <td>0.355668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.581926</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>0.877079</td>\n",
       "      <td>0.052273</td>\n",
       "      <td>0.321057</td>\n",
       "      <td>0.325521</td>\n",
       "      <td>1.040785</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.288106</td>\n",
       "      <td>0.082017</td>\n",
       "      <td>0.314247</td>\n",
       "      <td>0.353284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.123016</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>0.326754</td>\n",
       "      <td>0.080138</td>\n",
       "      <td>0.336647</td>\n",
       "      <td>0.375894</td>\n",
       "      <td>1.023775</td>\n",
       "      <td>0.003355</td>\n",
       "      <td>0.279837</td>\n",
       "      <td>0.079379</td>\n",
       "      <td>0.307204</td>\n",
       "      <td>0.353992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_loss  val_rpn_class_loss  val_rpn_bbox_loss  val_mrcnn_class_loss  \\\n",
       "0   1.559037            0.006998           0.524973              0.155440   \n",
       "1   1.536923            0.005690           0.389247              0.166668   \n",
       "2   1.266506            0.004292           0.442891              0.101443   \n",
       "3   1.628055            0.007729           0.790306              0.077846   \n",
       "4   1.769657            0.008058           0.888261              0.065135   \n",
       "5   1.292980            0.003645           0.338385              0.134752   \n",
       "6   1.137150            0.003925           0.320562              0.091569   \n",
       "7   1.272166            0.003899           0.434949              0.079931   \n",
       "8   1.470298            0.005524           0.613948              0.053944   \n",
       "9   1.169950            0.003847           0.346621              0.115049   \n",
       "10  1.327212            0.004178           0.423819              0.106955   \n",
       "11  1.313162            0.004971           0.330364              0.168774   \n",
       "12  1.219781            0.004627           0.388499              0.094531   \n",
       "13  1.470429            0.005969           0.476210              0.104199   \n",
       "14  1.277091            0.003263           0.358205              0.093495   \n",
       "15  1.306181            0.004869           0.503184              0.128378   \n",
       "16  1.293567            0.004487           0.396711              0.108286   \n",
       "17  1.217158            0.005325           0.371032              0.087607   \n",
       "18  1.581926            0.005989           0.877079              0.052273   \n",
       "19  1.123016            0.003575           0.326754              0.080138   \n",
       "\n",
       "    val_mrcnn_bbox_loss  val_mrcnn_mask_loss      loss  rpn_class_loss  \\\n",
       "0              0.472899             0.398720  1.627178        0.008232   \n",
       "1              0.538154             0.437156  1.339134        0.005704   \n",
       "2              0.358257             0.359615  1.301982        0.005006   \n",
       "3              0.390278             0.361887  1.237034        0.004815   \n",
       "4              0.393145             0.415052  1.208559        0.004360   \n",
       "5              0.436658             0.379533  1.165507        0.004371   \n",
       "6              0.347974             0.373113  1.170505        0.003818   \n",
       "7              0.372579             0.380801  1.186856        0.004057   \n",
       "8              0.404428             0.392447  1.161433        0.004388   \n",
       "9              0.348811             0.355614  1.118763        0.004042   \n",
       "10             0.404976             0.387276  1.132341        0.003851   \n",
       "11             0.402856             0.406189  1.133428        0.004209   \n",
       "12             0.363277             0.368839  1.097999        0.003776   \n",
       "13             0.445033             0.439010  1.121878        0.003742   \n",
       "14             0.416985             0.405136  1.080598        0.003447   \n",
       "15             0.331120             0.338623  1.044755        0.003629   \n",
       "16             0.397661             0.386413  1.062506        0.003362   \n",
       "17             0.384959             0.368228  1.086805        0.003483   \n",
       "18             0.321057             0.325521  1.040785        0.003124   \n",
       "19             0.336647             0.375894  1.023775        0.003355   \n",
       "\n",
       "    rpn_bbox_loss  mrcnn_class_loss  mrcnn_bbox_loss  mrcnn_mask_loss  \n",
       "0        0.515597          0.175124         0.522818         0.405400  \n",
       "1        0.408866          0.124896         0.421435         0.378224  \n",
       "2        0.406349          0.113762         0.400441         0.376416  \n",
       "3        0.376350          0.102587         0.380855         0.372419  \n",
       "4        0.371225          0.089148         0.368944         0.374875  \n",
       "5        0.349712          0.088900         0.355226         0.367290  \n",
       "6        0.357518          0.085255         0.354488         0.369419  \n",
       "7        0.339044          0.101099         0.364740         0.377908  \n",
       "8        0.364094          0.083685         0.346381         0.362877  \n",
       "9        0.333691          0.079176         0.339988         0.361858  \n",
       "10       0.330630          0.094548         0.340662         0.362642  \n",
       "11       0.327471          0.097593         0.342382         0.361766  \n",
       "12       0.313833          0.085535         0.334100         0.360748  \n",
       "13       0.327246          0.086727         0.335324         0.368830  \n",
       "14       0.301585          0.087673         0.327861         0.360024  \n",
       "15       0.282868          0.081565         0.321409         0.355275  \n",
       "16       0.303931          0.078861         0.320375         0.355969  \n",
       "17       0.313893          0.089518         0.324235         0.355668  \n",
       "18       0.288106          0.082017         0.314247         0.353284  \n",
       "19       0.279837          0.079379         0.307204         0.353992  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
